{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import akshare as ak\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每只股票的基础量价特征会保存在这里（如何获取数据及预处理详见factor_generate部分）\n",
    "df = pd.read_pickle('Factor_final.pkl')\n",
    "df = df.dropna(how='any',axis=0).reset_index(drop=True)\n",
    "df = df[df['instrument'].isin(ticker_list)]\n",
    "\n",
    "# 将全体样本根据根据股票代码进行拆分储存\n",
    "def iter_ticker_sample(one_tick):\n",
    "    lookback = 10\n",
    "    if len(one_tick) <= 2*lookback:\n",
    "        pass\n",
    "    else:\n",
    "        # one_tick = df[df['instrument']==ticker]\n",
    "        ticker = np.unique(one_tick['instrument'])[0]\n",
    "        one_tick = one_tick.sort_values(by='date')\n",
    "        head = one_tick[['date','instrument','label','label_raw','group']]\n",
    "        number = one_tick.drop(columns=['date','instrument','label','label_raw','group'])\n",
    "\n",
    "        def iter_flatten_sample(one_window):\n",
    "            return pd.DataFrame(one_window.values.flatten(),\n",
    "                        columns = [one_window.index[-1]]).T\n",
    "        col_base = number.columns.tolist()\n",
    "        col_new = []\n",
    "        for i in range(lookback):\n",
    "            col_new += [col+ f'_{-9+i}' for col in col_base]\n",
    "        len(col_new)\n",
    "\n",
    "        flatten_list = []\n",
    "        for i in range(lookback,len(one_tick)):\n",
    "            flatten_list.append(iter_flatten_sample(number.iloc[i-lookback:i,:]))\n",
    "        flatten_df = pd.concat(flatten_list,axis=0)\n",
    "        flatten_df.columns = col_new\n",
    "        flatten_df = pd.concat([head,flatten_df],axis=1).dropna(how='any',axis=0)\n",
    "        # flatten_df = pd.concat([flatten_df[['date','instrument']],flatten_df.drop(columns=['date','instrument']).astype('float32')],axis=1)\n",
    "        flatten_df.to_pickle(f'D:\\jupyter notebook\\graduate\\data_byticker\\{ticker}.pkl')\n",
    "        # flatten_df.to_parquet(f'D:\\jupyter notebook\\graduate\\data_byticker\\{ticker}.parquet')\n",
    "    return \n",
    "\n",
    "\n",
    "group = []\n",
    "for i in df.groupby(by='instrument'):\n",
    "    group.append(i[1]) \n",
    "ticker_sample = Parallel(n_jobs=16,verbose=1)(delayed(iter_ticker_sample)(one_tick) for one_tick in group)\n",
    "\n",
    "file_all = os.listdir('D:\\jupyter notebook\\graduate\\data_byticker')\n",
    "ticker_list = [filename.replace('.pkl','') for filename in file_all]\n",
    "\n",
    "# 再将样本根据日期重新排列\n",
    "file_all = os.listdir('data_bydate/')\n",
    "filename = file_all[1]\n",
    "for filename in tqdm(file_all):\n",
    "    date = filename.replace('.pkl','')\n",
    "    df1 = pd.read_pickle(f'data_bydate/{filename}')\n",
    "    # df1 = pd.read_pickle('data_bydate/20100322.pkl')\n",
    "    df1 = df1.dropna(how='any',axis=0)\n",
    "    df1['label'] = df1['label_raw'].rank(pct=True)\n",
    "    feature = df1.iloc[:,5:]\n",
    "    n_sample = feature.values.shape[0]\n",
    "    lookback = 10\n",
    "    n_ft = 126\n",
    "    feature = feature.values.reshape(n_sample,lookback,n_ft)\n",
    "    label = df1['label'].values.reshape(n_sample,1)\n",
    "    label_raw = df1['label_raw'].values.reshape(n_sample,1)\n",
    "    index = df1['instrument'].values.reshape(n_sample,1)\n",
    "    # save_path\n",
    "    np.save(f'D:/jupyter notebook/graduate/all_sample/feature/{date}.npy',feature)\n",
    "    np.save(f'D:/jupyter notebook/graduate/all_sample/label/{date}.npy',label)\n",
    "    np.save(f'D:/jupyter notebook/graduate/all_sample/index/{date}.npy',index)\n",
    "    np.save(f'D:/jupyter notebook/graduate/all_sample/label_raw/{date}.npy',label_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序神经网络模型设计与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 截取样本内\n",
    "date_list = os.listdir('data_bydate')\n",
    "date_list = [d.replace('.pkl','') for d in date_list]\n",
    "date_list.sort()\n",
    "\n",
    "end_year = 2021 # 选择样本内停止的年份，并固定会选取过去三年长度截取\n",
    "insample_end = str(datetime.date(end_year,1,1)).replace('-','')\n",
    "insample_start = str(datetime.date(end_year-3,1,1)).replace('-','')\n",
    "\n",
    "date_series = pd.Series(date_list)\n",
    "insample_date = date_series[(date_series > insample_start) & (date_series < insample_end)].tolist()\n",
    "train_date = insample_date[:-120] \n",
    "val_date = insample_date[-120:]\n",
    "\n",
    "X_train = [np.load(f'all_sample/feature/{date}.npy') for date in train_date]\n",
    "X_train = np.concatenate(X_train,axis=0)\n",
    "\n",
    "y_train = [np.load(f'all_sample/label/{date}.npy') for date in train_date]\n",
    "y_train = np.concatenate(y_train,axis=0)\n",
    "\n",
    "batch_list = [np.load(f'all_sample/feature/{date}.npy').shape[0] for date in train_date]\n",
    "batch_size = int(np.mean(batch_list))\n",
    "print(batch_size)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络部分\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.as_tensor(self.features[idx,:,:], dtype=torch.float32)   \n",
    "        y = torch.as_tensor(self.targets[idx], dtype=torch.float32)  \n",
    "        return x, y\n",
    "\n",
    "train_dataset = MyDataset(np.clip(X_train,-5,5),y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=False,num_workers=0,pin_memory=True)\n",
    "\n",
    "# 自定义网络结构，但核心在于必须有hookout提取中间层输出（以LSTM为例）\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        input_size, hidden_size, dropout = config[\"input_size\"], config[\"hidden_size\"], config[\"dropout\"] \n",
    "        num_layers = config[\"num_layers\"]\n",
    "        # hook out\n",
    "        self.hook_output = None\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # input shape: [batch * seq_len * d_model]\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.BatchNorm2d(hidden_size,eps=1e-05, momentum=0.1, affine=True)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.batchn = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(hidden_size,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self._device = config[\"device\"]\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        lstm_out = self.lstm(x)[0]\n",
    "        # detach to perform truncated bptt\n",
    "        # choose the latest layer\n",
    "        lstm_out = lstm_out[:,-1,:]\n",
    "        lstm_out = self.batchn(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        self.hook_output = lstm_out\n",
    "        # drop_out = self.dropout(lstm_out)\n",
    "        final_out = self.linear(lstm_out)\n",
    "        final_out = self.sigmoid(final_out)\n",
    " \n",
    "        return final_out\n",
    "    \n",
    "\n",
    "# 定义一个可以设置随机种子的函数\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    " \n",
    "# 设置随机数种子\n",
    "setup_seed(1)\n",
    "\n",
    "# 设置训练参数\n",
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "params = dict(\n",
    "    input_size = X_train.shape[-1],\n",
    "    hidden_size = 32,\n",
    "    dropout = 0.3,\n",
    "    num_layers = 1,\n",
    "    lr= 1e-4,\n",
    "    device=device,\n",
    "    output_size = y_train.shape[1]\n",
    ")\n",
    "\n",
    "lr = 1e-4\n",
    "train_end = None\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印网络结构\n",
    "def model_structure(model):\n",
    "    blank = ' '\n",
    "    print('-' * 90)\n",
    "    print('|' + ' ' * 11 + 'weight name' + ' ' * 10 + '|' \\\n",
    "          + ' ' * 15 + 'weight shape' + ' ' * 15 + '|' \\\n",
    "          + ' ' * 3 + 'number' + ' ' * 3 + '|')\n",
    "    print('-' * 90)\n",
    "    num_para = 0\n",
    "    type_size = 1  # 如果是浮点数就是4\n",
    "    outputlist = []\n",
    "    for index, (key, w_variable) in enumerate(model.named_parameters()):\n",
    "        if len(key) <= 30:\n",
    "            key = key + (30 - len(key)) * blank\n",
    "        shape = str(w_variable.shape)\n",
    "        if len(shape) <= 40:\n",
    "            shape = shape + (40 - len(shape)) * blank\n",
    "        each_para = 1\n",
    "        for k in w_variable.shape:\n",
    "            each_para *= k\n",
    "        num_para += each_para\n",
    "        str_num = str(each_para)\n",
    "        if len(str_num) <= 10:\n",
    "            str_num = str_num + (10 - len(str_num)) * blank\n",
    "\n",
    "        print('| {} | {} | {} |'.format(key, shape, str_num))\n",
    "        outputlist.append([key, shape, str_num])\n",
    "    print('-' * 90)\n",
    "    print('The total number of parameters: ' + str(num_para))\n",
    "    print('The parameters of Model {}: {:4f}M'.format(model._get_name(), num_para * type_size / 1000 / 1000))\n",
    "    print('-' * 90)\n",
    "    return outputlist\n",
    "\n",
    "my_lstm = LSTM_Model(params)\n",
    "structure_list = model_structure(my_lstm)\n",
    "latex_txt = pd.DataFrame(structure_list,\n",
    "             columns = ['weight name', 'weight shape', 'params number']).to_latex()\n",
    "print(latex_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "my_lstm = LSTM_Model(params).to(device)\n",
    "optimizer = torch.optim.Adam(my_lstm.parameters(),lr=lr)\n",
    "epoch = 20\n",
    "# loss_func = nn.MSELoss()\n",
    "\n",
    "def ic_loss(x,y,hook_output):\n",
    "    ft_size = hook_output.size()[-1]\n",
    "    rtn_corr = torch.corrcoef(torch.concat([x,y],axis=1).T)[0][1]\n",
    "    ft_corr = (torch.corrcoef(hook_output.T).abs().sum() - ft_size) / (ft_size * (ft_size-1))\n",
    "    return -rtn_corr + 0.1 * ft_corr \n",
    "\n",
    "loss_func = ic_loss\n",
    "\n",
    "for step in tqdm(range(epoch)):\n",
    "    print(\"\\n{:-^120s}\".format(f'epoch = {step}'))\n",
    "    loss_all = []\n",
    "    for tx, ty in train_loader:\n",
    "        # my_lstm = torch.load('best_model.pt', map_location=torch.device(device))\n",
    "        tx = tx.to(device)\n",
    "        ty = ty.to(device)\n",
    "\n",
    "        output = my_lstm(tx).to(device)\n",
    "        hook_output = my_lstm.hook_output\n",
    "        loss = loss_func(output,ty, hook_output)\n",
    "        # print(loss.item)\n",
    "        loss_all.append(loss.item())\n",
    "        # loss = ic_loss(output,ty)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(np.mean(loss_all))\n",
    "    torch.save(my_lstm,f'model_save/model_{insample_start}_{insample_end}_epoch{step}.pt')\n",
    "    my_lstm = my_lstm.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取训练结果\n",
    "def iter_predict_get(date):\n",
    "    device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_test = np.load(f'all_sample/feature/{date}.npy')\n",
    "    # y_test = np.load(f'all_sample/label/{date}.npy')\n",
    "    y_pred_list = []\n",
    "    for step in range(epoch):\n",
    "        # my_lstm_predict = LSTM_Model(params_predict)\n",
    "        my_lstm = torch.load(f'model_save/model_{insample_start}_{insample_end}_epoch{step}.pt').to(device)\n",
    "        # my_lstm_predict.load_state_dict(my_lstm.state_dict())\n",
    "        \n",
    "        y_pred = my_lstm(torch.as_tensor(X_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "        # my_lstm.hookout\n",
    "        y_pred_list.append(y_pred)\n",
    "    y_pred_all = np.concatenate(y_pred_list,axis=1)\n",
    "    torch.cuda.empty_cache()\n",
    "    return y_pred_all\n",
    "\n",
    "def iter_hookout_get(date,step):\n",
    "    device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_test = np.load(f'all_sample/feature/{date}.npy')\n",
    "\n",
    "    my_lstm = torch.load(f'model_save/model_{insample_start}_{insample_end}_epoch{step}.pt').to(device)\n",
    "\n",
    "    my_lstm(torch.as_tensor(X_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "    hook_output = my_lstm.hook_output.detach().cpu().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "    return hook_output\n",
    "\n",
    "def iter_predict_get_one(date,step):\n",
    "    device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_test = np.load(f'all_sample/feature/{date}.npy')\n",
    "\n",
    "    my_lstm = torch.load(f'model_save/model_{insample_start}_{insample_end}_epoch{step}.pt').to(device)\n",
    "    y_pred = my_lstm(torch.as_tensor(X_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "    return y_pred\n",
    "\n",
    "epoch = 20\n",
    "end_year = 2021\n",
    "# 指定日期信息\n",
    "insample_end = str(datetime.date(end_year,1,1)).replace('-','')\n",
    "insample_start = str(datetime.date(end_year-3,1,1)).replace('-','')\n",
    "outsample_end = str(datetime.date(end_year+1,1,1)).replace('-','')\n",
    "\n",
    "date_series = pd.Series(date_list)\n",
    "insample_date = date_series[(date_series > insample_start) & (date_series < insample_end)].tolist()\n",
    "outsample_date = date_series[(date_series > insample_end) & (date_series < outsample_end)].tolist()\n",
    "\n",
    "train_date = insample_date[:-120] \n",
    "val_date = insample_date[-120:]\n",
    "# val_date = val_date + outsample_date\n",
    "val_date.sort()\n",
    "out_date = val_date + outsample_date\n",
    "out_date.sort()\n",
    "\n",
    "# 提取预测值和中间层输出\n",
    "res = [iter_predict_get_one(date, 12) for date in tqdm(out_date)]\n",
    "hookout_res = [iter_hookout_get(date, 12) for date in tqdm(out_date)]\n",
    "\n",
    "y_train = [np.load(f'all_sample/label_raw/{date}.npy') for date in train_date]\n",
    "y_train = np.concatenate(y_train,axis=0)\n",
    "\n",
    "y_val = [np.load(f'all_sample/label_raw/{date}.npy') for date in val_date]\n",
    "y_val = np.concatenate(y_val,axis=0)\n",
    "\n",
    "y_pred_df = pd.DataFrame(np.concatenate(res,axis=0))\n",
    "\n",
    "y_train_pred = y_pred_df.head(y_train.shape[0])\n",
    "y_train_pred['base'] = y_train\n",
    "\n",
    "y_val_pred = y_pred_df.tail(y_val.shape[0])\n",
    "y_val_pred['base'] = y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析不同epoch训练效果\n",
    "from matplotlib.ticker import MaxNLocator \n",
    "train_ic = y_train_pred.corr(method='spearman')['base']\n",
    "val_ic = y_val_pred.corr(method='spearman')['base']\n",
    "\n",
    "ic_compare = pd.concat([train_ic,val_ic],axis=1).iloc[:-1,:]\n",
    "ic_compare.columns = ['train','val']\n",
    "ic_compare.plot(figsize=(10,5))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 观察中间层输出分布\n",
    "hookout_res = [iter_hookout_get(date, 17) for date in tqdm(insample_date)]\n",
    "\n",
    "hookout_df = pd.DataFrame(np.concatenate(hookout_res,axis=0))\n",
    "ft_size = hookout_df.shape[-1]\n",
    "hookout_tmp = hookout_df.head(y_train.shape[0])\n",
    "hookout_tmp.replace(0.,np.nan).hist(figsize=(20,20),bins=50)\n",
    "plt.show()\n",
    "\n",
    "hookout_tmp = hookout_df.tail(y_val.shape[0])\n",
    "hookout_tmp.replace(0.,np.nan).hist(figsize=(20,20),bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计中间层输出效果\n",
    "# 与预测目标之间的IC\n",
    "train_hookout = hookout_df.head(y_train.shape[0])\n",
    "train_hookout['label'] = y_train\n",
    "train_ft_ic = train_hookout.corr('spearman')['label'].iloc[:-1]\n",
    "train_ft_ic.plot(kind='bar',figsize=(10,5))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 中间层自身的相关性矩阵\n",
    "train_ft_corr = hookout_df.head(y_train.shape[0]).corr()\n",
    "print((train_ft_corr.abs().values.sum() - ft_size) / (ft_size * (ft_size -1)))\n",
    "\n",
    "f = plt.figure(figsize=(20,20))\n",
    "f.add_subplot(111)\n",
    "sns.heatmap(train_ft_corr,annot=True,square=True,cmap='RdBu_r',linewidths=0.3,vmax=1,vmin=-1,fmt='.2f',cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将中间层结果保存以备lgm处理\n",
    "idx_full_list = []\n",
    "for date in out_date:\n",
    "    ticker_list = np.load(f'all_sample/index/{date}.npy',allow_pickle=True)\n",
    "    idx_full = []\n",
    "    for i in ticker_list:\n",
    "        idx_full.append([date,i[0]])\n",
    "    idx_full_list.append(np.array(idx_full))\n",
    "idx_full = np.concatenate(idx_full_list,axis=0)\n",
    "\n",
    "y_test = [np.load(f'all_sample/label/{date}.npy') for date in out_date]\n",
    "y_test = np.concatenate(y_test,axis=0)\n",
    "\n",
    "rtn_test = [np.load(f'all_sample/label_raw/{date}.npy') for date in out_date]\n",
    "rtn_test = np.concatenate(rtn_test,axis=0)\n",
    "\n",
    "y_pred_df = pd.DataFrame(np.concatenate(res,axis=0),columns=['nn_pred'])\n",
    "y_pred_df[['date','id']] = idx_full\n",
    "y_pred_df['label'] = y_test\n",
    "y_pred_df['label_raw'] = rtn_test\n",
    "hookout_df = pd.DataFrame(np.concatenate(hookout_res,axis=0))\n",
    "df = pd.concat([y_pred_df,hookout_df],axis=1)\n",
    "# df.to_pickle(f'lgbm_data/{out_date[0]}_{out_date[-1]}_trans.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成学习模型LGBM再加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取特定区间的中间层输出\n",
    "df = pd.read_pickle('lgbm_data/20120710_20131231.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分lgm模型的样本内外\n",
    "insample_len = 120\n",
    "step = 10\n",
    "ft_num = 32\n",
    "\n",
    "date_list = df['date'].unique().tolist()\n",
    "date_list.sort()\n",
    "\n",
    "move_group = (len(date_list) - insample_len) // step + 2\n",
    "start_list = []\n",
    "for g in range(move_group):\n",
    "    i = g * step\n",
    "    start_list.append(i)\n",
    "    if i + insample_len + step >= len(date_list):\n",
    "        break\n",
    "\n",
    "print(start_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgm模型设计与训练\n",
    "params = {\n",
    "    'objective': 'rmse',  \n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_jobs': -1,\n",
    "    'verbose':-1,\n",
    "    'bagging_fraction':0.8,\n",
    "    'num_leaves':10,\n",
    "    'seed':2,\n",
    "    # 'learning_rate'\n",
    "}\n",
    "step = 20\n",
    "res_list = []\n",
    "out_sample_all = []\n",
    "for start in start_list:\n",
    "    train_date = date_list[start:start+insample_len] \n",
    "    val_date = train_date[-20:]\n",
    "    train_date = train_date[:-20]\n",
    "    test_date = date_list[date_list.index(train_date[-1]) + 1: date_list.index(train_date[-1]) + 1 + step]\n",
    "    print(len(train_date), len(val_date), len(test_date))\n",
    "    print(test_date)\n",
    "\n",
    "    train_df = df[df['date'].isin(train_date)]\n",
    "    val_df = df[df['date'].isin(val_date)]\n",
    "    test_df = df[df['date'].isin(test_date)]\n",
    "\n",
    "    x_train = train_df[[i for i in range(ft_num)]].values\n",
    "    y_train = train_df['label'].values\n",
    "    # x_train = torch.sigmoid(torch.tensor(x_train)).numpy()\n",
    "\n",
    "    x_val = val_df[[i for i in range(ft_num)]].values\n",
    "    y_val = val_df['label'].values\n",
    "    # x_val = torch.sigmoid(torch.tensor(x_val)).numpy()\n",
    "\n",
    "    x_test = test_df[[i for i in range(ft_num)]].values\n",
    "    y_test = test_df['label'].values\n",
    "    # x_test = torch.sigmoid(torch.tensor(x_test)).numpy()\n",
    "\n",
    "    def correlation(a, train_data):\n",
    "        \n",
    "        b = train_data.get_label()\n",
    "        \n",
    "        a = np.ravel(a)\n",
    "        b = np.ravel(b)\n",
    "\n",
    "        len_data = len(a)\n",
    "        mean_a = np.sum(a) / len_data\n",
    "        mean_b = np.sum(b) / len_data\n",
    "        var_a = np.sum(np.square(a - mean_a)) / len_data\n",
    "        var_b = np.sum(np.square(b - mean_b)) / len_data\n",
    "\n",
    "        cov = np.sum((a * b))/len_data - mean_a*mean_b\n",
    "        corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "        return 'corr', corr, True\n",
    "\n",
    "    train_dataset = lgb.Dataset(x_train, y_train)\n",
    "    val_dataset = lgb.Dataset(x_val, y_val)\n",
    "    model = lgb.train(params = params, \n",
    "                        train_set = train_dataset, \n",
    "                        valid_sets = [val_dataset], \n",
    "                        num_boost_round = 50, \n",
    "                        early_stopping_rounds = 5, \n",
    "                        \n",
    "                        verbose_eval = False,\n",
    "                        feval = correlation)\n",
    "\n",
    "    y_predict = model.predict(x_test)\n",
    "    test_df['lgm_pred'] = y_predict\n",
    "    out_sample_all.append(test_df)\n",
    "    res_tmp = test_df[['nn_pred','lgm_pred','label_raw']].corr('spearman')\n",
    "    res_list.append(res_tmp.tail(1))\n",
    "    display(res_tmp)\n",
    "# latex_txt = pd.concat(res_list,axis=0).reset_index(drop=True).head(10).drop(columns=['label_raw']).T.to_latex()\n",
    "# print(latex_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计lgm模型给出的特征重要性\n",
    "params = {\n",
    "    'objective': 'rmse',  \n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_jobs': -1,\n",
    "    'verbose':-1,\n",
    "    'bagging_fraction':0.8,\n",
    "    'num_leaves':10,\n",
    "    'seed':2,\n",
    "    # 'learning_rate'\n",
    "}\n",
    "\n",
    "def correlation(a, train_data):\n",
    "    b = train_data.get_label()\n",
    "    a = np.ravel(a)\n",
    "    b = np.ravel(b)\n",
    "    len_data = len(a)\n",
    "    mean_a = np.sum(a) / len_data\n",
    "    mean_b = np.sum(b) / len_data\n",
    "    var_a = np.sum(np.square(a - mean_a)) / len_data\n",
    "    var_b = np.sum(np.square(b - mean_b)) / len_data\n",
    "\n",
    "    cov = np.sum((a * b))/len_data - mean_a*mean_b\n",
    "    corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "    return 'corr', corr, True\n",
    "\n",
    "train_dataset = lgb.Dataset(x_train, y_train)\n",
    "val_dataset = lgb.Dataset(x_val, y_val)\n",
    "model = lgb.train(params = params, \n",
    "                    train_set = train_dataset, \n",
    "                    valid_sets = [val_dataset], \n",
    "                    num_boost_round = 50, \n",
    "                    early_stopping_rounds = 5, \n",
    "                    \n",
    "                    verbose_eval = False,\n",
    "                    feval = correlation)\n",
    "\n",
    "ft_importance = pd.DataFrame([model.feature_importance(importance_type='split')\n",
    "                             ,model.feature_importance(importance_type='gain')]).T\n",
    "ft_importance.columns = ['split','gain']\n",
    "ft_importance\n",
    "\n",
    "ic_importance = train_df.drop(columns=['nn_pred','date','id','label']).corr('spearman')['label_raw'].iloc[1:]\n",
    "ft_importance['ic'] = ic_importance.abs().values\n",
    "\n",
    "ft_importance.rank(axis=0).plot(kind='bar',figsize=(10,5))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比基准模型与lgm模型之间的预测效果\n",
    "full_testdf = pd.concat(out_sample_all,axis=0)\n",
    "\n",
    "test_nn = full_testdf.pivot_table(index='date',columns='id',values='nn_pred')\n",
    "test_lgm = full_testdf.pivot_table(index='date',columns='id',values='lgm_pred')\n",
    "\n",
    "test_rtn = full_testdf.pivot_table(index='date',columns='id',values='label_raw')\n",
    "test_nn_ic = test_rtn.corrwith(test_nn,axis=1,method='spearman')\n",
    "test_nn_ic.index = pd.to_datetime(test_nn_ic.index)\n",
    "test_nn_ic.plot()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "test_lgm_ic = test_rtn.corrwith(test_lgm,axis=1,method='spearman')\n",
    "test_lgm_ic.index = pd.to_datetime(test_lgm_ic.index)\n",
    "test_lgm_ic.plot()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "test_nn_ic.mean() / test_nn_ic.std()\n",
    "\n",
    "compare_ic = pd.concat([test_lgm_ic,test_nn_ic],axis=1)\n",
    "compare_ic = compare_ic.loc['20130101':,:]\n",
    "compare_ic.columns = ['lgbm','nn']\n",
    "compare_ic.plot(figsize=(10,5)).fill_between(compare_ic.index, 0.03, -0.03,color='yellow',alpha=0.4)\n",
    "plt.axhline(0.03,color='black',linestyle='--')\n",
    "plt.axhline(-0.03,color='black',linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 策略设计与回测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取基准指数信息\n",
    "sh01 = ak.stock_zh_index_daily(symbol=\"sh000001\")\n",
    "sz02 = ak.stock_zh_index_daily(symbol=\"sz000002\")\n",
    "sh01['amount'] = sh01['close'] * sh01['volume']\n",
    "sz02['amount'] = sz02['close'] * sz02['volume']\n",
    "\n",
    "sh01.index = sh01.date\n",
    "sz02.index = sz02.date\n",
    "sh01['rtn'] = sh01['open'].pct_change(1).shift(-2)\n",
    "sz02['rtn'] = sz02['open'].pct_change(1).shift(-2)\n",
    "\n",
    "lack = sh01.loc[sh01.index.difference(sz02.index),:]\n",
    "sz02 = pd.concat([sz02,lack],axis=0).sort_index()\n",
    "\n",
    "full_market_rtn = (sz02['amount'] * sz02['rtn'] + sh01['amount'] * sh01['rtn']) / (sh01['amount'] + sz02['amount'])\n",
    "full_market_rtn = full_market_rtn.to_frame('full_rtn')\n",
    "full_market_rtn.index = full_market_rtn.index.astype('str').map(lambda x: x.replace('-',''))\n",
    "\n",
    "# 回测\n",
    "from FactorAnalysis import * \n",
    "df_factor = test_lgm.loc['20130101':,:]\n",
    "df_rtn = test_rtn.loc['20130101':,:]\n",
    "benchmark = full_market_rtn.loc[df_rtn.index,:]\n",
    "benchmark.index = pd.to_datetime(benchmark.index)\n",
    "fa_sub = FactorAnalysis(df_factor,df_rtn,[1],10,benchmark=benchmark,ret_kind='sum',cost=0.0008)\n",
    "fa_sub.fast_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
